{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe1c676-34ce-451c-a47a-17c1b60a1bd1",
   "metadata": {},
   "source": [
    "# ResNet Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae932817-6e92-428c-8744-bb9afd8db71b",
   "metadata": {},
   "source": [
    "## ResNet 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4afc8f8-d58b-46d8-8154-06fe80ff527e",
   "metadata": {},
   "source": [
    "#### Libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f92f6e3c-48d8-41b7-a0a7-8347f15bfcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.ao.quantization.fake_quantize import FakeQuantize\n",
    "from torch.ao.quantization.observer import HistogramObserver, MovingAverageMinMaxObserver, MovingAveragePerChannelMinMaxObserver\n",
    "from torch.ao.quantization.qconfig import QConfig\n",
    "import torch.ao.quantization as quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33beff62-fb58-420b-a710-ec2c7ee8ce4a",
   "metadata": {},
   "source": [
    "#### Dataset and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f266189a-0388-4f03-91ac-b3c8d4463589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset size: 100001\n",
      "The number of classes: 1000\n"
     ]
    }
   ],
   "source": [
    "data_dir = './ImageNet1k/imagenet1k'\n",
    "transform = transforms.Compose(\n",
    "                [\n",
    "                    #transforms.Resize(224),\n",
    "                    transforms.RandomResizedCrop(224),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "\n",
    "train_datasets = datasets.ImageFolder(os.path.join(data_dir), transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=32, shuffle=True, num_workers=6)\n",
    "print('Validation dataset size:', len(train_datasets))\n",
    "\n",
    "class_names = train_datasets.classes\n",
    "print('The number of classes:', len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a905839d-0413-43ce-851e-583f7391342a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset size: 5000\n",
      "The number of classes: 1000\n"
     ]
    }
   ],
   "source": [
    "data_dir = './Small-ImageNet1k/ILSVRC2012_img_val_subset'\n",
    "transform = transforms.Compose(\n",
    "                [\n",
    "                    #transforms.Resize(224),\n",
    "                    transforms.RandomResizedCrop(224),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "\n",
    "val_datasets = datasets.ImageFolder(os.path.join(data_dir), transform)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_datasets, batch_size=16, shuffle=True, num_workers=6)\n",
    "print('Validation dataset size:', len(val_datasets))\n",
    "\n",
    "class_names = val_datasets.classes\n",
    "print('The number of classes:', len(class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c481f3ea-8078-4ed8-8136-8b2085340957",
   "metadata": {},
   "source": [
    "#### ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54166e58-96a1-4e3d-8fb1-9018cdbd9e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(weights = torchvision.models.ResNet18_Weights.IMAGENET1K_V1, progress = True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d937e20b-bdc5-478a-87f5-644b903d5be6",
   "metadata": {},
   "source": [
    "#### Fine-Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61af6bc-b1b6-40b5-b93c-6acd5bdafb03",
   "metadata": {},
   "source": [
    "Tras numerosas pruebas, se ha concluido que el proceso de finetunning es efectivo para mellorar la precisión de la red planteada pero no merece la pena en comparación al tiempo que lleva (+-2 horas). Por ello, se plantea dejar esta sección sin ejecutar si se están realizando pruebas sobre el resto del Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e87bf43-5e6e-4614-b087-ebacfac45c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting=True):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "set_parameter_requires_grad(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fac180c5-f642-4f89-a95e-ec3e72315fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 1000, bias = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc933ed4-97c3-4541-b861-485567125b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optimizer, criteria, epoch, log_interval=len(train_dataloader.dataset)):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criteria(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(images), len(train_dataloader.dataset), 100. * batch_idx / len(train_dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f94560-0c97-42c3-8a07-0acd9784a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train(optimizer, criteria, epoch)\n",
    "    #validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344459d7-eda0-44ce-95a3-025dd47330a3",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd4ae729-76f4-4704-b3b2-994be1212141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model: 44.666MB\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict() , './models/ResNet18/original_model.pth')\n",
    "\n",
    "float_model_size = os.path.getsize('./models/ResNet18/original_model.pth') / 1024**2\n",
    "print(\"Float model: {:.3f}MB\".format(float_model_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee80f794-3dcd-4758-8a75-4ad38ca9e2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brais.martinez/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(weights = torch.load('./models/ResNet18/original_model.pth', weights_only = True), progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d83839f5-7917-488d-827f-7a9219acfcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(network, criteria):\n",
    "    network.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, basic_labels) in enumerate(val_dataloader):\n",
    "\n",
    "            labels = torch.zeros_like(basic_labels)\n",
    "            for j in range(labels.shape[0]):\n",
    "                labels[j] = int(class_names[basic_labels[j]])\n",
    "            \n",
    "            output = network(inputs)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            \n",
    "            test_loss += criteria(output, labels).item() * inputs.size(0)\n",
    "            correct += preds.eq(labels.data.view_as(preds)).sum()\n",
    "            \n",
    "    test_loss /= len(val_dataloader.dataset)\n",
    "    print('\\nTest set: Avg. loss: {:.6f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(test_loss, correct, len(val_dataloader.dataset), 0100. * correct / len(val_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "803f1427-9c24-4d0a-8586-c9dbe724b7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 1.702150, Accuracy: 3091/5000 (61.82%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e214e4d-1f85-4e22-9ea7-112866655c27",
   "metadata": {},
   "source": [
    "#### Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dfa1db-ab58-48bc-99fc-50b4506de9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models.quantization as qt\n",
    "quant_model = qt.resnet18(weights = qt.ResNet18_QuantizedWeights.IMAGENET1K_FBGEMM_V1, quantize = True)\n",
    "quant_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e527ea5-e5b3-4b75-aa20-70f800d6cb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model: 11.293MB\n"
     ]
    }
   ],
   "source": [
    "torch.save(quant_model.state_dict() , './models/ResNet18/original_quant_model.pth')\n",
    "\n",
    "model_size = os.path.getsize('./models/ResNet18/original_quant_model.pth') / 1024**2\n",
    "print(\"Float model: {:.3f}MB\".format(model_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7a74db5-ecef-4fec-b207-f268f737fffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 1.702848, Accuracy: 3090/5000 (61.80%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(quant_model, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea81f8d-6b2c-44d6-9c53-533f08c94e78",
   "metadata": {},
   "source": [
    "#### Post-Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c28b3b67-a61e-4172-8933-7d12ce13cfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptq_model_8bit = torch.quantization.quantize_dynamic(model, {nn.Linear, nn.Conv2d, nn.BatchNorm2d}, dtype=torch.qint8)\n",
    "torch.save(ptq_model_8bit.state_dict(), './models/ResNet18/ptq_model_8bit.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a94413d-6a41-4248-a3db-70a7a813502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model: 43.202MB\n"
     ]
    }
   ],
   "source": [
    "model_size = os.path.getsize('./models/ResNet18/ptq_model_8bit.pth') / 1024**2\n",
    "print(\"Float model: {:.3f}MB\".format(model_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c80953d-d838-4a82-a5f9-aac8265100e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 1.677211, Accuracy: 3088/5000 (61.76%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(quant_model, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c9b3d-2fc2-4f18-8dab-8a984978f8cf",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13edbf2-5aea-49c6-a2b7-444b2081b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet50(weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V1, progress = True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f20dfc-9ffe-4e9d-bbb7-83cd8c2782e1",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa8c58c3-d30f-43fe-b594-630ee5d05f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model: 97.793MB\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict() , './models/ResNet50/original_model.pth')\n",
    "\n",
    "float_model_size = os.path.getsize('./models/ResNet50/original_model.pth') / 1024**2\n",
    "print(\"Float model: {:.3f}MB\".format(float_model_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66870ec0-50e2-4843-b98f-c44c43555fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brais.martinez/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet50(weights = torch.load('./models/ResNet50/original_model.pth', weights_only = True), progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ead177f1-f834-45cd-9e7d-b6514768d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(network, criteria):\n",
    "    network.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, basic_labels) in enumerate(val_dataloader):\n",
    "\n",
    "            labels = torch.zeros_like(basic_labels)\n",
    "            for j in range(labels.shape[0]):\n",
    "                labels[j] = int(class_names[basic_labels[j]])\n",
    "            \n",
    "            output = network(inputs)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            \n",
    "            test_loss += criteria(output, labels).item() * inputs.size(0)\n",
    "            correct += preds.eq(labels.data.view_as(preds)).sum()\n",
    "            \n",
    "    test_loss /= len(val_dataloader.dataset)\n",
    "    print('\\nTest set: Avg. loss: {:.6f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(test_loss, correct, len(val_dataloader.dataset), 0100. * correct / len(val_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6aeaaee-29df-4c18-9681-4276a8579081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 1.299501, Accuracy: 3488/5000 (69.76%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804f8e5-a94d-4736-9256-9e684a3c42c3",
   "metadata": {},
   "source": [
    "#### Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36fac96-3716-4a02-a48f-a1d1b4e1c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models.quantization as qt\n",
    "quant_model = qt.resnet50(weights = qt.ResNet50_QuantizedWeights.IMAGENET1K_FBGEMM_V1, quantize = True)\n",
    "quant_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "387a6774-246f-41a9-8f97-e13cdaa867bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model: 24.957MB\n"
     ]
    }
   ],
   "source": [
    "torch.save(quant_model.state_dict() , './models/ResNet50/original_quant_model.pth')\n",
    "\n",
    "model_size = os.path.getsize('./models/ResNet50/original_quant_model.pth') / 1024**2\n",
    "print(\"Float model: {:.3f}MB\".format(model_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00229a5f-0219-4b73-8006-40eab0466dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 1.298116, Accuracy: 3492/5000 (69.84%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(quant_model, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ad824-37ed-4930-af08-01f9641fda57",
   "metadata": {},
   "source": [
    "#### Post-Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c65698eb-bdcd-43d6-8b5e-9d1cdfa6c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptq_model_8bit = torch.quantization.quantize_dynamic(model, {nn.Linear, nn.Conv2d, nn.BatchNorm2d}, dtype=torch.qint8)\n",
    "torch.save(ptq_model_8bit.state_dict(), './models/ResNet50/ptq_model_8bit.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acbaebd5-d3ac-480f-93c5-94d77863aedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model: 91.934MB\n"
     ]
    }
   ],
   "source": [
    "model_size = os.path.getsize('./models/ResNet50/ptq_model_8bit.pth') / 1024**2\n",
    "print(\"Float model: {:.3f}MB\".format(model_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dafd3c12-128c-4df8-a107-7ef0700012b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 1.325471, Accuracy: 3426/5000 (68.52%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(quant_model, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca5d50a-23ed-4769-a308-b5ac23fce03d",
   "metadata": {},
   "source": [
    "## ResNet 152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fbeaab-a545-47db-93fb-841cd6f56bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet152(weights = torchvision.models.ResNet152_Weights.IMAGENET1K_V1, progress = True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb744a9a-4798-4ba5-a1d9-9ab34a401e0b",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "408c9b4f-8f29-41ae-a5d1-56c89f84de04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model: 230.481MB\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict() , './models/ResNet152/original_model.pth')\n",
    "\n",
    "float_model_size = os.path.getsize('./models/ResNet152/original_model.pth') / 1024**2\n",
    "print(\"Float model: {:.3f}MB\".format(float_model_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50391629-5479-4052-8ecd-a4118e9a0ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brais.martinez/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet152(weights = torch.load('./models/ResNet152/original_model.pth', weights_only = True), progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6257da33-a2ca-44f4-b187-cdee8575106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(network, criteria):\n",
    "    network.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, basic_labels) in enumerate(val_dataloader):\n",
    "\n",
    "            labels = torch.zeros_like(basic_labels)\n",
    "            for j in range(labels.shape[0]):\n",
    "                labels[j] = int(class_names[basic_labels[j]])\n",
    "            \n",
    "            output = network(inputs)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            \n",
    "            test_loss += criteria(output, labels).item() * inputs.size(0)\n",
    "            correct += preds.eq(labels.data.view_as(preds)).sum()\n",
    "            \n",
    "    test_loss /= len(val_dataloader.dataset)\n",
    "    print('\\nTest set: Avg. loss: {:.6f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(test_loss, correct, len(val_dataloader.dataset), 0100. * correct / len(val_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22bee82e-2c09-4ce1-ade1-e21a100b3ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 1.220836, Accuracy: 3584/5000 (71.68%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae14658-0e8f-446e-b056-1c40007c6e78",
   "metadata": {},
   "source": [
    "#### Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "336765ab-0497-451b-bc11-517ce93edde1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.models.quantization' has no attribute 'resnet152'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mqt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m quant_model \u001b[38;5;241m=\u001b[39m \u001b[43mqt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet152\u001b[49m(weights \u001b[38;5;241m=\u001b[39m qt\u001b[38;5;241m.\u001b[39mResNet152_QuantizedWeights\u001b[38;5;241m.\u001b[39mIMAGENET1K_FBGEMM_V1, quantize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m quant_model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchvision.models.quantization' has no attribute 'resnet152'"
     ]
    }
   ],
   "source": [
    "import torchvision.models.quantization as qt\n",
    "quant_model = qt.resnet152(weights = qt.ResNet152_QuantizedWeights.IMAGENET1K_FBGEMM_V1, quantize = True)\n",
    "quant_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b6e8e8-0c03-4241-9775-a7e3728ec512",
   "metadata": {},
   "source": [
    "No se puede realizar este apartado debido a que el módulo de quantization propio de Pytorch únicamente se encuentra para la ResNet18 y la ResNet50. El resto de ResNets no cuentan con cuantización propia de Pytorch, que es la que mejor resultados da, por lo que nos tendremos que conformar con la realizada a mano mediante los módulos de cuantización de 8 bits de Pytorch normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6a8f62-0de5-4398-861b-9b4334a835f6",
   "metadata": {},
   "source": [
    "#### Post-Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34c5c9b5-8636-4aeb-b32f-6312323531fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptq_model_8bit = torch.quantization.quantize_dynamic(model, {nn.Linear, nn.Conv2d, nn.BatchNorm2d}, dtype=torch.qint8)\n",
    "torch.save(ptq_model_8bit.state_dict(), './models/ResNet152/ptq_model_8bit.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32c1ac1b-9ecb-42a8-bda9-27d76dc80a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model: 224.622MB\n"
     ]
    }
   ],
   "source": [
    "model_size = os.path.getsize('./models/ResNet152/ptq_model_8bit.pth') / 1024**2\n",
    "print(\"Float model: {:.3f}MB\".format(model_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ebf5bb1c-39c9-4326-93cc-813aa60c5a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 1.317694, Accuracy: 3488/5000 (69.76%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(quant_model, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb2093d-a4d5-43a3-a4af-b6400fd1ee6d",
   "metadata": {},
   "source": [
    "# The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e36ea637-8af2-4187-a980-4311db3b7401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ඞ\n"
     ]
    }
   ],
   "source": [
    "print(chr(sum(range(ord(min(str(not())))))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
